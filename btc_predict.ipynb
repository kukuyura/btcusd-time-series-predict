{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Исследование опережающих корреляций для BTC\n",
    "\n",
    "Искомый датасет подготовлен в рамках курса \"Временные ряды для прогноза криптовалют\", дополнительная дата получена с Kaggle. Необходимо построить предсказание курса BTC для недели 8-14 июля 2023 на основе данных из ПРОШЛОГО. Мое решение удовлетворяет данному естественному требованию, в чем вы можете убедиться во время ознакомления с ноутбуком. Частично ориентировался на данную статью: https://cointelegraph.com/news/bitcoin-assets-most-correlated-assets-today и на этот ноутбук: https://github.com/tinkoff-ai/etna/blob/master/examples/NN_examples.ipynb\n",
    "\n",
    "Буду проверять наличие зависимостей с Золотом, Серебром, ETH и информацией с отдельно взятой биржи - Binance"
   ],
   "metadata": {
    "id": "4j-i7grbQMAg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Подготовка даты\n",
    "\n",
    "Для начала подключим нужные модули и загрузим main дату:"
   ],
   "metadata": {
    "id": "YSFK1svzQ5qj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#! pip install etna #--quiet\n",
    "#! pip install etna[torch] #--quiet"
   ],
   "metadata": {
    "id": "OlJtpzB8Wljo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5eb97d60-dfef-4bc0-9e4c-140e67f95674",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:27.954686030Z",
     "start_time": "2023-10-20T13:55:27.827734869Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader as DataLoader"
   ],
   "metadata": {
    "id": "JBxi1J7VSf90",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:28.375870637Z",
     "start_time": "2023-10-20T13:55:27.827928650Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from etna.analysis import plot_backtest\n",
    "from etna.datasets import TSDataset\n",
    "from etna.metrics import MAE, SMAPE\n",
    "from etna.models.nn.deepar import DeepARModel\n",
    "from etna.models.nn.tft import TFTModel\n",
    "from etna.pipeline import Pipeline\n",
    "\n",
    "from etna.transforms import DateFlagsTransform, HolidayTransform, LagTransform, StandardScalerTransform"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "veY811Bw3QA6",
    "outputId": "401c61cd-ca67-4287-cf65-499119139f9c",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:29.794253529Z",
     "start_time": "2023-10-20T13:55:28.377233734Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/settings.py:45: UserWarning: wandb is not available, to install it, run `pip install etna[wandb]`\n",
      "  warnings.warn(\"wandb is not available, to install it, run `pip install etna[wandb]`\")\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/settings.py:53: UserWarning: etna[prophet] is not available, to install it, run `pip install etna[prophet]`\n",
      "  warnings.warn(\"etna[prophet] is not available, to install it, run `pip install etna[prophet]`\")\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/settings.py:62: UserWarning: etna[classification] is not available, to install it, run `pip install etna[classification]`\n",
      "  warnings.warn(\"etna[classification] is not available, to install it, run `pip install etna[classification]`\")\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/settings.py:79: UserWarning: etna[statsforecast] is not available, to install it, run `pip install etna[statsforecast]`\n",
      "  warnings.warn(\"etna[statsforecast] is not available, to install it, run `pip install etna[statsforecast]`\")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!wget  -O 'Bitcoin_kaggle.csv' -q 'https://www.dropbox.com/scl/fi/gv5l69x4btxy0525x8vz6/Bitcoin_kaggle.csv?rlkey=r9it4a8ruwiv4vjnwx08cpgr1&dl=0'"
   ],
   "metadata": {
    "id": "IGjv6oJLQHjT",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:32.229924920Z",
     "start_time": "2023-10-20T13:55:29.795331132Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ilx68rlKPqPa",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:32.290221758Z",
     "start_time": "2023-10-20T13:55:32.237696245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               Price      Open      High       Low    Vol. Change %\nDate                                                               \n2023-07-14  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-13  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-12  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-11  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-10  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-09  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-08  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-07  30,346.4  29,912.7  30,442.0  29,757.4  46.42K    1.45%\n2023-07-06  29,913.1  30,512.8  31,463.6  29,869.0  90.81K   -1.97%\n2023-07-05  30,512.8  30,768.6  30,875.6  30,233.3  43.23K   -0.83%",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Vol.</th>\n      <th>Change %</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-07-14</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-13</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-12</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-11</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-10</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-09</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-08</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-07</th>\n      <td>30,346.4</td>\n      <td>29,912.7</td>\n      <td>30,442.0</td>\n      <td>29,757.4</td>\n      <td>46.42K</td>\n      <td>1.45%</td>\n    </tr>\n    <tr>\n      <th>2023-07-06</th>\n      <td>29,913.1</td>\n      <td>30,512.8</td>\n      <td>31,463.6</td>\n      <td>29,869.0</td>\n      <td>90.81K</td>\n      <td>-1.97%</td>\n    </tr>\n    <tr>\n      <th>2023-07-05</th>\n      <td>30,512.8</td>\n      <td>30,768.6</td>\n      <td>30,875.6</td>\n      <td>30,233.3</td>\n      <td>43.23K</td>\n      <td>-0.83%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"Bitcoin_kaggle.csv\"\n",
    "df_main = pd.read_csv(path, delimiter=';', parse_dates=True, index_col='Date')\n",
    "df_main.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Последние семь записей дублируют восьмую, так как цену на них и нужно предсказать. Пока удалим их:"
   ],
   "metadata": {
    "id": "QtUFmsfgWsTB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_main = df_main[7:]"
   ],
   "metadata": {
    "id": "CfsAcasHTDcN",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:32.293223083Z",
     "start_time": "2023-10-20T13:55:32.291821794Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Еще немного преобразований для main даты:"
   ],
   "metadata": {
    "id": "dcvMMgWvfmKZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for col in ['Price', 'Open', 'High', 'Low']:\n",
    "    df_main[col] = df_main[col].apply(lambda x: float(x.replace(\",\",\"\")))\n",
    "df_main['Vol.'] = df_main['Vol.'].replace({\"K\":\"*1e3\", \"M\":\"*1e6\", 'B':'*1e9'}, regex=True).map(pd.eval)\n",
    "# df_main['Change %'] = df_main['Change %'].apply(lambda x: float(x.replace(\"%\",\"\")))\n",
    "df_main = df_main.drop(columns=['Change %'])"
   ],
   "metadata": {
    "id": "SoJR6xHafl2Z",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:32.560493646Z",
     "start_time": "2023-10-20T13:55:32.293396004Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Добавим возможность использовать дополнительную дату:"
   ],
   "metadata": {
    "id": "BDTO3o66XzMP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget  -O 'gold.csv' -q 'https://www.dropbox.com/scl/fi/p3yjrmll7g18nmqgzq1pm/gold-prices.csv?rlkey=b839f09hctmo23i5ib9e0dtgc&dl=0'\n",
    "!wget  -O 'silver.csv' -q 'https://www.dropbox.com/scl/fi/d2zpyn9emm41sespi2l16/silver-prices.csv?rlkey=pudjpwq1hbykgr0efh0yvyayv&dl=0'\n",
    "!wget  -O 'eth.csv' -q 'https://www.dropbox.com/scl/fi/9aj9srozkhs87c8710bjz/ETH-USD.csv?rlkey=byw2txha1gpie78vj9rep2rgu&dl=0'\n",
    "!wget  -O 'binance.csv' -q 'https://www.dropbox.com/scl/fi/x9p9senmyn28s66uh56tq/BTCUSDT_binance.csv?rlkey=o8whyy43j02dsryt7dd581p2x&dl=0'"
   ],
   "metadata": {
    "id": "ovy0SWyCW8VM",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:41.932883167Z",
     "start_time": "2023-10-20T13:55:32.560719673Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Определим первую и последнюю дату в main датасете:"
   ],
   "metadata": {
    "id": "qqgOeBnAcGvF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "first_index = df_main.head(1).index[0]\n",
    "last_index = df_main.tail(1).index[0]\n",
    "print(first_index)\n",
    "print(last_index)"
   ],
   "metadata": {
    "id": "tWH3g2lccGCY",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:41.939687355Z",
     "start_time": "2023-10-20T13:55:41.937140890Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-07 00:00:00\n",
      "2022-01-14 00:00:00\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def add_eth():\n",
    "  global df_main\n",
    "  path = \"eth.csv\"\n",
    "  df_tmp = pd.read_csv(path, delimiter=',', parse_dates=True, index_col='Date')\n",
    "  df_tmp = df_tmp.loc[::-1,:]\n",
    "  df_tmp = df_tmp.loc[first_index:last_index]\n",
    "  df_tmp = df_tmp.drop(columns=[\"Adj Close\"])\n",
    "  df_tmp.rename(columns={'Close': 'Close ETH',\n",
    "                         'Volume':'Volume ETH',\n",
    "                         'Open':  'Open ETH',\n",
    "                         'High':  'High ETH',\n",
    "                         'Low':   'Low ETH'}, inplace=True)\n",
    "  df_main = pd.concat([df_main, df_tmp], axis=1)\n",
    "\n",
    "\n",
    "def add_binance():\n",
    "    global df_main\n",
    "    path = \"binance.csv\"\n",
    "    df_tmp = pd.read_csv(path, delimiter=',')\n",
    "    df_tmp['Date'] = df_tmp['Open_Time'].apply(lambda x: datetime.utcfromtimestamp(x / 1000))\n",
    "    df_tmp = df_tmp.set_index(\"Date\")\n",
    "    df_tmp = df_tmp[[\"QAV\", \"NoT\", \"Taker buy base asset volume\", \"Taker buy quote asset volume\"]]\n",
    "    df_tmp = df_tmp.loc[::-1,:]\n",
    "    df_tmp = df_tmp.loc[first_index:last_index]\n",
    "    df_main = pd.concat([df_main, df_tmp], axis=1)\n",
    "\n",
    "def add_metal(metal : str):\n",
    "  global df_main\n",
    "  path = f\"{metal}.csv\"\n",
    "  df_tmp = pd.read_csv(path, delimiter=',', parse_dates=True, index_col='Date')\n",
    "  df_tmp = df_tmp.loc[first_index:last_index]\n",
    "  df_tmp.rename(columns={'Close/Last':f'Close {metal}',\n",
    "                         'Volume':    f'Volume {metal}',\n",
    "                         'Open':      f'Open {metal}',\n",
    "                         'High':      f'High {metal}',\n",
    "                         'Low':       f'Low {metal}'}, inplace=True)\n",
    "  df_main = pd.concat([df_main, df_tmp], axis=1)"
   ],
   "metadata": {
    "id": "EHCVXwxyX9UU",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:41.948543070Z",
     "start_time": "2023-10-20T13:55:41.942781448Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "add_eth()\n",
    "add_binance()\n",
    "#add_metal('gold')\n",
    "#add_metal('silver')\n",
    "df_main.head(10)"
   ],
   "metadata": {
    "id": "y63wgHXdhR5S",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:41.976017129Z",
     "start_time": "2023-10-20T13:55:41.949204524Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "              Price     Open     High      Low      Vol.     Open ETH  \\\nDate                                                                    \n2023-07-07  30346.4  29912.7  30442.0  29757.4   46420.0  1847.512573   \n2023-07-06  29913.1  30512.8  31463.6  29869.0   90810.0  1910.417114   \n2023-07-05  30512.8  30768.6  30875.6  30233.3   43230.0  1936.796753   \n2023-07-04  30768.4  31152.0  31326.5  30657.8   42120.0  1955.524170   \n2023-07-03  31151.3  30617.5  31377.0  30581.5   56490.0  1937.883789   \n2023-07-02  30617.7  30587.1  30769.0  30227.9   28820.0  1924.448120   \n2023-07-01  30586.8  30472.9  30649.9  30329.0   22460.0  1933.323853   \n2023-06-30  30472.9  30445.7  31275.5  29714.5  118650.0  1852.008423   \n2023-06-29  30445.7  30077.3  30823.1  30051.3   49570.0  1828.059326   \n2023-06-28  30078.6  30691.9  30703.4  29919.5   51060.0  1889.906494   \n\n               High ETH      Low ETH    Close ETH   Volume ETH           QAV  \\\nDate                                                                           \n2023-07-07  1876.963257  1832.025391  1870.602539   6468885150  1.027462e+09   \n2023-07-06  1956.012329  1847.850708  1848.636475   8905008384  2.181764e+09   \n2023-07-05  1942.432495  1897.124756  1910.588013   6034088075  1.013695e+09   \n2023-07-04  1966.365356  1932.611328  1936.633545   5683423776  1.028794e+09   \n2023-07-03  1974.775024  1934.688843  1955.389160   7858509087  1.352635e+09   \n2023-07-02  1958.160767  1895.906982  1937.438354   6343966490  7.110314e+08   \n2023-07-01  1942.701538  1910.848633  1924.565918   5136809625  5.338593e+08   \n2023-06-30  1945.274292  1831.281006  1933.188965  12895131248  2.726356e+09   \n2023-06-29  1876.530273  1828.059326  1852.227295   5677228612  1.107940e+09   \n2023-06-28  1890.208984  1822.102783  1827.971191   7135265016  1.224104e+09   \n\n                NoT  Taker buy base asset volume  Taker buy quote asset volume  \nDate                                                                            \n2023-07-07   912948                  14875.86309                  4.486388e+08  \n2023-07-06  1397042                  33952.63237                  1.039137e+09  \n2023-07-05   827926                  15619.11716                  4.764783e+08  \n2023-07-04   779472                  14107.36566                  4.371262e+08  \n2023-07-03   781855                  20885.94331                  6.456987e+08  \n2023-07-02   591533                  10630.24740                  3.246160e+08  \n2023-07-01   567975                   8102.13876                  2.471751e+08  \n2023-06-30  1559366                  44319.81254                  1.351735e+09  \n2023-06-29   758428                  17834.38857                  5.438727e+08  \n2023-06-28   895470                  19176.75428                  5.802267e+08  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Vol.</th>\n      <th>Open ETH</th>\n      <th>High ETH</th>\n      <th>Low ETH</th>\n      <th>Close ETH</th>\n      <th>Volume ETH</th>\n      <th>QAV</th>\n      <th>NoT</th>\n      <th>Taker buy base asset volume</th>\n      <th>Taker buy quote asset volume</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-07-07</th>\n      <td>30346.4</td>\n      <td>29912.7</td>\n      <td>30442.0</td>\n      <td>29757.4</td>\n      <td>46420.0</td>\n      <td>1847.512573</td>\n      <td>1876.963257</td>\n      <td>1832.025391</td>\n      <td>1870.602539</td>\n      <td>6468885150</td>\n      <td>1.027462e+09</td>\n      <td>912948</td>\n      <td>14875.86309</td>\n      <td>4.486388e+08</td>\n    </tr>\n    <tr>\n      <th>2023-07-06</th>\n      <td>29913.1</td>\n      <td>30512.8</td>\n      <td>31463.6</td>\n      <td>29869.0</td>\n      <td>90810.0</td>\n      <td>1910.417114</td>\n      <td>1956.012329</td>\n      <td>1847.850708</td>\n      <td>1848.636475</td>\n      <td>8905008384</td>\n      <td>2.181764e+09</td>\n      <td>1397042</td>\n      <td>33952.63237</td>\n      <td>1.039137e+09</td>\n    </tr>\n    <tr>\n      <th>2023-07-05</th>\n      <td>30512.8</td>\n      <td>30768.6</td>\n      <td>30875.6</td>\n      <td>30233.3</td>\n      <td>43230.0</td>\n      <td>1936.796753</td>\n      <td>1942.432495</td>\n      <td>1897.124756</td>\n      <td>1910.588013</td>\n      <td>6034088075</td>\n      <td>1.013695e+09</td>\n      <td>827926</td>\n      <td>15619.11716</td>\n      <td>4.764783e+08</td>\n    </tr>\n    <tr>\n      <th>2023-07-04</th>\n      <td>30768.4</td>\n      <td>31152.0</td>\n      <td>31326.5</td>\n      <td>30657.8</td>\n      <td>42120.0</td>\n      <td>1955.524170</td>\n      <td>1966.365356</td>\n      <td>1932.611328</td>\n      <td>1936.633545</td>\n      <td>5683423776</td>\n      <td>1.028794e+09</td>\n      <td>779472</td>\n      <td>14107.36566</td>\n      <td>4.371262e+08</td>\n    </tr>\n    <tr>\n      <th>2023-07-03</th>\n      <td>31151.3</td>\n      <td>30617.5</td>\n      <td>31377.0</td>\n      <td>30581.5</td>\n      <td>56490.0</td>\n      <td>1937.883789</td>\n      <td>1974.775024</td>\n      <td>1934.688843</td>\n      <td>1955.389160</td>\n      <td>7858509087</td>\n      <td>1.352635e+09</td>\n      <td>781855</td>\n      <td>20885.94331</td>\n      <td>6.456987e+08</td>\n    </tr>\n    <tr>\n      <th>2023-07-02</th>\n      <td>30617.7</td>\n      <td>30587.1</td>\n      <td>30769.0</td>\n      <td>30227.9</td>\n      <td>28820.0</td>\n      <td>1924.448120</td>\n      <td>1958.160767</td>\n      <td>1895.906982</td>\n      <td>1937.438354</td>\n      <td>6343966490</td>\n      <td>7.110314e+08</td>\n      <td>591533</td>\n      <td>10630.24740</td>\n      <td>3.246160e+08</td>\n    </tr>\n    <tr>\n      <th>2023-07-01</th>\n      <td>30586.8</td>\n      <td>30472.9</td>\n      <td>30649.9</td>\n      <td>30329.0</td>\n      <td>22460.0</td>\n      <td>1933.323853</td>\n      <td>1942.701538</td>\n      <td>1910.848633</td>\n      <td>1924.565918</td>\n      <td>5136809625</td>\n      <td>5.338593e+08</td>\n      <td>567975</td>\n      <td>8102.13876</td>\n      <td>2.471751e+08</td>\n    </tr>\n    <tr>\n      <th>2023-06-30</th>\n      <td>30472.9</td>\n      <td>30445.7</td>\n      <td>31275.5</td>\n      <td>29714.5</td>\n      <td>118650.0</td>\n      <td>1852.008423</td>\n      <td>1945.274292</td>\n      <td>1831.281006</td>\n      <td>1933.188965</td>\n      <td>12895131248</td>\n      <td>2.726356e+09</td>\n      <td>1559366</td>\n      <td>44319.81254</td>\n      <td>1.351735e+09</td>\n    </tr>\n    <tr>\n      <th>2023-06-29</th>\n      <td>30445.7</td>\n      <td>30077.3</td>\n      <td>30823.1</td>\n      <td>30051.3</td>\n      <td>49570.0</td>\n      <td>1828.059326</td>\n      <td>1876.530273</td>\n      <td>1828.059326</td>\n      <td>1852.227295</td>\n      <td>5677228612</td>\n      <td>1.107940e+09</td>\n      <td>758428</td>\n      <td>17834.38857</td>\n      <td>5.438727e+08</td>\n    </tr>\n    <tr>\n      <th>2023-06-28</th>\n      <td>30078.6</td>\n      <td>30691.9</td>\n      <td>30703.4</td>\n      <td>29919.5</td>\n      <td>51060.0</td>\n      <td>1889.906494</td>\n      <td>1890.208984</td>\n      <td>1822.102783</td>\n      <td>1827.971191</td>\n      <td>7135265016</td>\n      <td>1.224104e+09</td>\n      <td>895470</td>\n      <td>19176.75428</td>\n      <td>5.802267e+08</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Во время локальных тестов металлы не помогли, в отличие от ETH и информации с Binance. Более того, данные за выходные о торговле металлами полностью состоят из NaN-ов. Методы загрузки соответствующей даты оставлю для энтузиастов."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_main = df_main.dropna()"
   ],
   "metadata": {
    "id": "OApp76-8oFpB",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:41.979432707Z",
     "start_time": "2023-10-20T13:55:41.977222716Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_main.rename(columns={'Price': 'target', 'Date': 'timestamp', 'Vol.': 'Vol'}, inplace=True)\n",
    "df_main['segment']='main'\n",
    "df_main['timestamp'] = df_main.index\n",
    "df_target = TSDataset.to_dataset(df=df_main[[\"target\", \"segment\", \"timestamp\"]])\n",
    "df_target = df_target.dropna()\n",
    "df_target.head(10)"
   ],
   "metadata": {
    "id": "hmLydCYNwpLJ",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:42.092426123Z",
     "start_time": "2023-10-20T13:55:41.980911336Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "segment        main\nfeature      target\ntimestamp          \n2022-01-14  43073.3\n2022-01-15  43097.0\n2022-01-16  43079.1\n2022-01-17  42209.3\n2022-01-18  42364.6\n2022-01-19  41677.8\n2022-01-20  40715.9\n2022-01-21  36475.5\n2022-01-22  35075.2\n2022-01-23  36269.5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>segment</th>\n      <th>main</th>\n    </tr>\n    <tr>\n      <th>feature</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-01-14</th>\n      <td>43073.3</td>\n    </tr>\n    <tr>\n      <th>2022-01-15</th>\n      <td>43097.0</td>\n    </tr>\n    <tr>\n      <th>2022-01-16</th>\n      <td>43079.1</td>\n    </tr>\n    <tr>\n      <th>2022-01-17</th>\n      <td>42209.3</td>\n    </tr>\n    <tr>\n      <th>2022-01-18</th>\n      <td>42364.6</td>\n    </tr>\n    <tr>\n      <th>2022-01-19</th>\n      <td>41677.8</td>\n    </tr>\n    <tr>\n      <th>2022-01-20</th>\n      <td>40715.9</td>\n    </tr>\n    <tr>\n      <th>2022-01-21</th>\n      <td>36475.5</td>\n    </tr>\n    <tr>\n      <th>2022-01-22</th>\n      <td>35075.2</td>\n    </tr>\n    <tr>\n      <th>2022-01-23</th>\n      <td>36269.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df_exog = TSDataset.to_dataset(df=df_main.drop(columns=[\"target\"]))\n",
    "df_exog.head(10)"
   ],
   "metadata": {
    "id": "MOgoP7WKxa6K",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:42.092645119Z",
     "start_time": "2023-10-20T13:55:42.028973605Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "segment            main                                                       \\\nfeature       Close ETH     High     High ETH      Low      Low ETH      NoT   \ntimestamp                                                                      \n2022-01-14  3310.001465  43435.1  3330.766113  41848.7  3203.823730   946938   \n2022-01-15  3330.530762  43777.9  3364.537842  42586.1  3278.670898   752688   \n2022-01-16  3350.921875  43462.0  3376.401123  42643.3  3291.563721   732171   \n2022-01-17  3212.304932  43179.6  3355.819336  41559.4  3157.224121   918029   \n2022-01-18  3164.025146  42674.2  3236.016113  41300.7  3096.123535   905061   \n2022-01-19  3095.825928  42558.0  3171.158447  41160.9  3055.212402   924528   \n2022-01-20  3001.120117  43487.1  3265.336914  40568.3  3000.908203  1098761   \n2022-01-21  2557.931641  41104.6  3029.081055  35503.9  2496.812988  2092561   \n2022-01-22  2405.181152  36749.8  2615.247314  34116.0  2330.247314  2099978   \n2022-01-23  2535.039063  36513.0  2542.144775  34655.2  2381.515137  1142407   \n\nsegment                                                                     \\\nfeature        Open     Open ETH           QAV Taker buy base asset volume   \ntimestamp                                                                    \n2022-01-14  42562.2  3248.648682  1.392000e+09                15434.189450   \n2022-01-15  43073.6  3309.844238  9.464883e+08                11133.186150   \n2022-01-16  43079.2  3330.387207  8.872031e+08                10052.192910   \n2022-01-17  43080.5  3350.947266  1.169382e+09                13448.541920   \n2022-01-18  42209.9  3212.287598  1.228460e+09                14527.823710   \n2022-01-19  42365.3  3163.850342  1.327478e+09                15603.743040   \n2022-01-20  41683.6  3095.271729  1.784801e+09                20201.479910   \n2022-01-21  40698.1  3002.956787  3.405502e+09                41615.085819   \n2022-01-22  36467.7  2561.145264  3.207531e+09                42722.278233   \n2022-01-23  35072.9  2406.924316  1.572415e+09                22841.394510   \n\nsegment                                                         \nfeature    Taker buy quote asset volume       Vol   Volume ETH  \ntimestamp                                                       \n2022-01-14                 6.584724e+08   50500.0  13562957230  \n2022-01-15                 4.805165e+08   31440.0   9619999078  \n2022-01-16                 4.328999e+08   28660.0   9505934874  \n2022-01-17                 5.706905e+08   41440.0  12344309617  \n2022-01-18                 6.085790e+08   47320.0  13024154091  \n2022-01-19                 6.538076e+08   53770.0  13187424144  \n2022-01-20                 8.526295e+08   65280.0  10645922764  \n2022-01-21                 1.595832e+09  155800.0  26796291874  \n2022-01-22                 1.514027e+09  138090.0  27369692036  \n2022-01-23                 8.115015e+08   70430.0  16481489511  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>segment</th>\n      <th colspan=\"13\" halign=\"left\">main</th>\n    </tr>\n    <tr>\n      <th>feature</th>\n      <th>Close ETH</th>\n      <th>High</th>\n      <th>High ETH</th>\n      <th>Low</th>\n      <th>Low ETH</th>\n      <th>NoT</th>\n      <th>Open</th>\n      <th>Open ETH</th>\n      <th>QAV</th>\n      <th>Taker buy base asset volume</th>\n      <th>Taker buy quote asset volume</th>\n      <th>Vol</th>\n      <th>Volume ETH</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-01-14</th>\n      <td>3310.001465</td>\n      <td>43435.1</td>\n      <td>3330.766113</td>\n      <td>41848.7</td>\n      <td>3203.823730</td>\n      <td>946938</td>\n      <td>42562.2</td>\n      <td>3248.648682</td>\n      <td>1.392000e+09</td>\n      <td>15434.189450</td>\n      <td>6.584724e+08</td>\n      <td>50500.0</td>\n      <td>13562957230</td>\n    </tr>\n    <tr>\n      <th>2022-01-15</th>\n      <td>3330.530762</td>\n      <td>43777.9</td>\n      <td>3364.537842</td>\n      <td>42586.1</td>\n      <td>3278.670898</td>\n      <td>752688</td>\n      <td>43073.6</td>\n      <td>3309.844238</td>\n      <td>9.464883e+08</td>\n      <td>11133.186150</td>\n      <td>4.805165e+08</td>\n      <td>31440.0</td>\n      <td>9619999078</td>\n    </tr>\n    <tr>\n      <th>2022-01-16</th>\n      <td>3350.921875</td>\n      <td>43462.0</td>\n      <td>3376.401123</td>\n      <td>42643.3</td>\n      <td>3291.563721</td>\n      <td>732171</td>\n      <td>43079.2</td>\n      <td>3330.387207</td>\n      <td>8.872031e+08</td>\n      <td>10052.192910</td>\n      <td>4.328999e+08</td>\n      <td>28660.0</td>\n      <td>9505934874</td>\n    </tr>\n    <tr>\n      <th>2022-01-17</th>\n      <td>3212.304932</td>\n      <td>43179.6</td>\n      <td>3355.819336</td>\n      <td>41559.4</td>\n      <td>3157.224121</td>\n      <td>918029</td>\n      <td>43080.5</td>\n      <td>3350.947266</td>\n      <td>1.169382e+09</td>\n      <td>13448.541920</td>\n      <td>5.706905e+08</td>\n      <td>41440.0</td>\n      <td>12344309617</td>\n    </tr>\n    <tr>\n      <th>2022-01-18</th>\n      <td>3164.025146</td>\n      <td>42674.2</td>\n      <td>3236.016113</td>\n      <td>41300.7</td>\n      <td>3096.123535</td>\n      <td>905061</td>\n      <td>42209.9</td>\n      <td>3212.287598</td>\n      <td>1.228460e+09</td>\n      <td>14527.823710</td>\n      <td>6.085790e+08</td>\n      <td>47320.0</td>\n      <td>13024154091</td>\n    </tr>\n    <tr>\n      <th>2022-01-19</th>\n      <td>3095.825928</td>\n      <td>42558.0</td>\n      <td>3171.158447</td>\n      <td>41160.9</td>\n      <td>3055.212402</td>\n      <td>924528</td>\n      <td>42365.3</td>\n      <td>3163.850342</td>\n      <td>1.327478e+09</td>\n      <td>15603.743040</td>\n      <td>6.538076e+08</td>\n      <td>53770.0</td>\n      <td>13187424144</td>\n    </tr>\n    <tr>\n      <th>2022-01-20</th>\n      <td>3001.120117</td>\n      <td>43487.1</td>\n      <td>3265.336914</td>\n      <td>40568.3</td>\n      <td>3000.908203</td>\n      <td>1098761</td>\n      <td>41683.6</td>\n      <td>3095.271729</td>\n      <td>1.784801e+09</td>\n      <td>20201.479910</td>\n      <td>8.526295e+08</td>\n      <td>65280.0</td>\n      <td>10645922764</td>\n    </tr>\n    <tr>\n      <th>2022-01-21</th>\n      <td>2557.931641</td>\n      <td>41104.6</td>\n      <td>3029.081055</td>\n      <td>35503.9</td>\n      <td>2496.812988</td>\n      <td>2092561</td>\n      <td>40698.1</td>\n      <td>3002.956787</td>\n      <td>3.405502e+09</td>\n      <td>41615.085819</td>\n      <td>1.595832e+09</td>\n      <td>155800.0</td>\n      <td>26796291874</td>\n    </tr>\n    <tr>\n      <th>2022-01-22</th>\n      <td>2405.181152</td>\n      <td>36749.8</td>\n      <td>2615.247314</td>\n      <td>34116.0</td>\n      <td>2330.247314</td>\n      <td>2099978</td>\n      <td>36467.7</td>\n      <td>2561.145264</td>\n      <td>3.207531e+09</td>\n      <td>42722.278233</td>\n      <td>1.514027e+09</td>\n      <td>138090.0</td>\n      <td>27369692036</td>\n    </tr>\n    <tr>\n      <th>2022-01-23</th>\n      <td>2535.039063</td>\n      <td>36513.0</td>\n      <td>2542.144775</td>\n      <td>34655.2</td>\n      <td>2381.515137</td>\n      <td>1142407</td>\n      <td>35072.9</td>\n      <td>2406.924316</td>\n      <td>1.572415e+09</td>\n      <td>22841.394510</td>\n      <td>8.115015e+08</td>\n      <td>70430.0</td>\n      <td>16481489511</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_ts = TSDataset(df=df_target, df_exog=df_exog,freq=\"D\")\n",
    "train_ts.head(10)"
   ],
   "metadata": {
    "id": "vuaLPSCtxppK",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:55:42.093115574Z",
     "start_time": "2023-10-20T13:55:42.029195135Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "segment            main                                                       \\\nfeature       Close ETH     High     High ETH      Low      Low ETH      NoT   \ntimestamp                                                                      \n2022-01-14  3310.001465  43435.1  3330.766113  41848.7  3203.823730   946938   \n2022-01-15  3330.530762  43777.9  3364.537842  42586.1  3278.670898   752688   \n2022-01-16  3350.921875  43462.0  3376.401123  42643.3  3291.563721   732171   \n2022-01-17  3212.304932  43179.6  3355.819336  41559.4  3157.224121   918029   \n2022-01-18  3164.025146  42674.2  3236.016113  41300.7  3096.123535   905061   \n2022-01-19  3095.825928  42558.0  3171.158447  41160.9  3055.212402   924528   \n2022-01-20  3001.120117  43487.1  3265.336914  40568.3  3000.908203  1098761   \n2022-01-21  2557.931641  41104.6  3029.081055  35503.9  2496.812988  2092561   \n2022-01-22  2405.181152  36749.8  2615.247314  34116.0  2330.247314  2099978   \n2022-01-23  2535.039063  36513.0  2542.144775  34655.2  2381.515137  1142407   \n\nsegment                                                                     \\\nfeature        Open     Open ETH           QAV Taker buy base asset volume   \ntimestamp                                                                    \n2022-01-14  42562.2  3248.648682  1.392000e+09                15434.189450   \n2022-01-15  43073.6  3309.844238  9.464883e+08                11133.186150   \n2022-01-16  43079.2  3330.387207  8.872031e+08                10052.192910   \n2022-01-17  43080.5  3350.947266  1.169382e+09                13448.541920   \n2022-01-18  42209.9  3212.287598  1.228460e+09                14527.823710   \n2022-01-19  42365.3  3163.850342  1.327478e+09                15603.743040   \n2022-01-20  41683.6  3095.271729  1.784801e+09                20201.479910   \n2022-01-21  40698.1  3002.956787  3.405502e+09                41615.085819   \n2022-01-22  36467.7  2561.145264  3.207531e+09                42722.278233   \n2022-01-23  35072.9  2406.924316  1.572415e+09                22841.394510   \n\nsegment                                                                  \nfeature    Taker buy quote asset volume       Vol   Volume ETH   target  \ntimestamp                                                                \n2022-01-14                 6.584724e+08   50500.0  13562957230  43073.3  \n2022-01-15                 4.805165e+08   31440.0   9619999078  43097.0  \n2022-01-16                 4.328999e+08   28660.0   9505934874  43079.1  \n2022-01-17                 5.706905e+08   41440.0  12344309617  42209.3  \n2022-01-18                 6.085790e+08   47320.0  13024154091  42364.6  \n2022-01-19                 6.538076e+08   53770.0  13187424144  41677.8  \n2022-01-20                 8.526295e+08   65280.0  10645922764  40715.9  \n2022-01-21                 1.595832e+09  155800.0  26796291874  36475.5  \n2022-01-22                 1.514027e+09  138090.0  27369692036  35075.2  \n2022-01-23                 8.115015e+08   70430.0  16481489511  36269.5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>segment</th>\n      <th colspan=\"14\" halign=\"left\">main</th>\n    </tr>\n    <tr>\n      <th>feature</th>\n      <th>Close ETH</th>\n      <th>High</th>\n      <th>High ETH</th>\n      <th>Low</th>\n      <th>Low ETH</th>\n      <th>NoT</th>\n      <th>Open</th>\n      <th>Open ETH</th>\n      <th>QAV</th>\n      <th>Taker buy base asset volume</th>\n      <th>Taker buy quote asset volume</th>\n      <th>Vol</th>\n      <th>Volume ETH</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-01-14</th>\n      <td>3310.001465</td>\n      <td>43435.1</td>\n      <td>3330.766113</td>\n      <td>41848.7</td>\n      <td>3203.823730</td>\n      <td>946938</td>\n      <td>42562.2</td>\n      <td>3248.648682</td>\n      <td>1.392000e+09</td>\n      <td>15434.189450</td>\n      <td>6.584724e+08</td>\n      <td>50500.0</td>\n      <td>13562957230</td>\n      <td>43073.3</td>\n    </tr>\n    <tr>\n      <th>2022-01-15</th>\n      <td>3330.530762</td>\n      <td>43777.9</td>\n      <td>3364.537842</td>\n      <td>42586.1</td>\n      <td>3278.670898</td>\n      <td>752688</td>\n      <td>43073.6</td>\n      <td>3309.844238</td>\n      <td>9.464883e+08</td>\n      <td>11133.186150</td>\n      <td>4.805165e+08</td>\n      <td>31440.0</td>\n      <td>9619999078</td>\n      <td>43097.0</td>\n    </tr>\n    <tr>\n      <th>2022-01-16</th>\n      <td>3350.921875</td>\n      <td>43462.0</td>\n      <td>3376.401123</td>\n      <td>42643.3</td>\n      <td>3291.563721</td>\n      <td>732171</td>\n      <td>43079.2</td>\n      <td>3330.387207</td>\n      <td>8.872031e+08</td>\n      <td>10052.192910</td>\n      <td>4.328999e+08</td>\n      <td>28660.0</td>\n      <td>9505934874</td>\n      <td>43079.1</td>\n    </tr>\n    <tr>\n      <th>2022-01-17</th>\n      <td>3212.304932</td>\n      <td>43179.6</td>\n      <td>3355.819336</td>\n      <td>41559.4</td>\n      <td>3157.224121</td>\n      <td>918029</td>\n      <td>43080.5</td>\n      <td>3350.947266</td>\n      <td>1.169382e+09</td>\n      <td>13448.541920</td>\n      <td>5.706905e+08</td>\n      <td>41440.0</td>\n      <td>12344309617</td>\n      <td>42209.3</td>\n    </tr>\n    <tr>\n      <th>2022-01-18</th>\n      <td>3164.025146</td>\n      <td>42674.2</td>\n      <td>3236.016113</td>\n      <td>41300.7</td>\n      <td>3096.123535</td>\n      <td>905061</td>\n      <td>42209.9</td>\n      <td>3212.287598</td>\n      <td>1.228460e+09</td>\n      <td>14527.823710</td>\n      <td>6.085790e+08</td>\n      <td>47320.0</td>\n      <td>13024154091</td>\n      <td>42364.6</td>\n    </tr>\n    <tr>\n      <th>2022-01-19</th>\n      <td>3095.825928</td>\n      <td>42558.0</td>\n      <td>3171.158447</td>\n      <td>41160.9</td>\n      <td>3055.212402</td>\n      <td>924528</td>\n      <td>42365.3</td>\n      <td>3163.850342</td>\n      <td>1.327478e+09</td>\n      <td>15603.743040</td>\n      <td>6.538076e+08</td>\n      <td>53770.0</td>\n      <td>13187424144</td>\n      <td>41677.8</td>\n    </tr>\n    <tr>\n      <th>2022-01-20</th>\n      <td>3001.120117</td>\n      <td>43487.1</td>\n      <td>3265.336914</td>\n      <td>40568.3</td>\n      <td>3000.908203</td>\n      <td>1098761</td>\n      <td>41683.6</td>\n      <td>3095.271729</td>\n      <td>1.784801e+09</td>\n      <td>20201.479910</td>\n      <td>8.526295e+08</td>\n      <td>65280.0</td>\n      <td>10645922764</td>\n      <td>40715.9</td>\n    </tr>\n    <tr>\n      <th>2022-01-21</th>\n      <td>2557.931641</td>\n      <td>41104.6</td>\n      <td>3029.081055</td>\n      <td>35503.9</td>\n      <td>2496.812988</td>\n      <td>2092561</td>\n      <td>40698.1</td>\n      <td>3002.956787</td>\n      <td>3.405502e+09</td>\n      <td>41615.085819</td>\n      <td>1.595832e+09</td>\n      <td>155800.0</td>\n      <td>26796291874</td>\n      <td>36475.5</td>\n    </tr>\n    <tr>\n      <th>2022-01-22</th>\n      <td>2405.181152</td>\n      <td>36749.8</td>\n      <td>2615.247314</td>\n      <td>34116.0</td>\n      <td>2330.247314</td>\n      <td>2099978</td>\n      <td>36467.7</td>\n      <td>2561.145264</td>\n      <td>3.207531e+09</td>\n      <td>42722.278233</td>\n      <td>1.514027e+09</td>\n      <td>138090.0</td>\n      <td>27369692036</td>\n      <td>35075.2</td>\n    </tr>\n    <tr>\n      <th>2022-01-23</th>\n      <td>2535.039063</td>\n      <td>36513.0</td>\n      <td>2542.144775</td>\n      <td>34655.2</td>\n      <td>2381.515137</td>\n      <td>1142407</td>\n      <td>35072.9</td>\n      <td>2406.924316</td>\n      <td>1.572415e+09</td>\n      <td>22841.394510</td>\n      <td>8.115015e+08</td>\n      <td>70430.0</td>\n      <td>16481489511</td>\n      <td>36269.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение\n",
    "\n",
    "Для начала зафиксируем гиперпараметры - я бы хотел сделать автоматический подбор, но моих мощностей не хватит"
   ],
   "metadata": {
    "id": "EPMryeyLxy0v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "SEED = 69\n",
    "HORIZON = 7\n",
    "N_FOLDS = 2\n",
    "MAX_EPOCHS = 100\n",
    "LR = 0.01\n",
    "# LAGS = 3\n",
    "# RNN_LAYERS = 2\n",
    "# GRADIENT_CLIP_VAL = 0.5"
   ],
   "metadata": {
    "id": "tlOSgXWWyEq4",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:59:25.736938544Z",
     "start_time": "2023-10-20T13:59:25.696013854Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ],
   "metadata": {
    "id": "pTceqiY7yLwK",
    "ExecuteTime": {
     "end_time": "2023-10-20T13:59:26.121909565Z",
     "start_time": "2023-10-20T13:59:26.113155538Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "И вот самое интересное - обучение. Смысл параметров очевиден из названий и кода. В качестве модели я буду использовать DeepAR, который есть в модуле ETNA. Локально пробовал запускать TFTModel, но получил более плохие результаты."
   ],
   "metadata": {
    "id": "Ho4MMOF5ySpY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "best_double_MAE = 2000\n",
    "best_pipeline = None\n",
    "best_forecast = None\n",
    "BEST_LAGS = None\n",
    "BEST_RNN_LAYERS = None\n",
    "BEST_GRADIENT_CLIP_VAL = None\n",
    "BEST_SHIFT = None\n",
    "\n",
    "for LAGS in [3, 5]:\n",
    "    for RNN_LAYERS in [1, 2]:\n",
    "        for GRADIENT_CLIP_VAL in [0.01, 0.1, 0.5]:\n",
    "            for SHIFT in [0, 1]:\n",
    "                lags_arr = [HORIZON + SHIFT + i for i in range(LAGS)]\n",
    "                lag_columns = [f\"target_lag_{lag}\" for lag in lags_arr]\n",
    "                transform_lag = LagTransform(in_column=\"target\",\n",
    "                                             lags=lags_arr,\n",
    "                                             out_column=\"target_lag\")\n",
    "                transform_date = DateFlagsTransform(day_number_in_week=True, day_number_in_month=False, out_column=\"dateflag\")\n",
    "                transform_holiday = HolidayTransform(out_column=\"holidayflag\")\n",
    "                transforms_arr = [transform_lag, transform_date, transform_holiday]\n",
    "\n",
    "                model = DeepARModel(encoder_length=HORIZON,\n",
    "                                    decoder_length=HORIZON,\n",
    "                                    trainer_params=dict(max_epochs=MAX_EPOCHS, gradient_clip_val=GRADIENT_CLIP_VAL),\n",
    "                                    lr=LR,\n",
    "                                    rnn_layers=RNN_LAYERS,\n",
    "                                    train_batch_size=64)\n",
    "\n",
    "                pipeline_kukuyura = Pipeline(model=model, horizon=HORIZON, transforms=transforms_arr)\n",
    "\n",
    "                metrics, forecast, _ = pipeline_kukuyura.backtest(train_ts, metrics=[MAE()], n_folds=N_FOLDS, n_jobs=1)\n",
    "\n",
    "                if metrics['MAE'].sum() < best_double_MAE:\n",
    "                    best_double_MAE = metrics['MAE'].sum()\n",
    "                    best_pipeline = pipeline_kukuyura\n",
    "                    best_forecast = forecast\n",
    "                    BEST_LAGS = LAGS\n",
    "                    BEST_RNN_LAYERS = RNN_LAYERS\n",
    "                    BEST_GRADIENT_CLIP_VAL = GRADIENT_CLIP_VAL\n",
    "                    BEST_SHIFT = SHIFT"
   ],
   "metadata": {
    "id": "9J5cH3ONySJR",
    "ExecuteTime": {
     "end_time": "2023-10-20T14:22:19.892495204Z",
     "start_time": "2023-10-20T13:59:27.466566415Z"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fcfc9a5f90f46a4b873a27933328740"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   21.5s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be9af27d9c434013a81b64acdd8f24ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   42.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   42.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565.524386160714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92619492eba14843976309fb2ae9b905"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.1s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "108fc471aff647dcafb5ecca550d9fa9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   47.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   47.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73544f57e5344bffaf7d2f5c8edffdf7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.9s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3431e06c9d448e9b351b350c8a72de2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   50.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   50.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1316.105189732143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7222345a48914ad2b3760a4f12fe8716"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.3s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb785c5bf33b4f149a19a5b53002ecfb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   46.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   46.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "804dd19bddf7437eb970c800a4f01c66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.0s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76b3d73d3d944afa9702c678faba5d89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a88d73dd413c4a7390c974ca666aa0d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.3s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d555b0142b624485b3734d419a490f67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   46.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   46.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943.2633928571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d0a7afcf1444aee8c3ff2fdb9e63684"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.0s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f08c720157284beaaf753b3b9d63c5b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "981ded6dc7024e2caabd77ca84576c44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.7s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b71b0f2685b342cea050e7fad5582a4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79fbc0b2443a4990924b92a3cab5b3c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.5s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d579428038647d08cc57cfadd015853"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c164f237a9b242e6877bf65b1c000956"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.8s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6eeadb527f6d4d8a8e7f49949b1c0687"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733.1952008928575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b9c18b18cf1468f8adff525fabe824c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.0s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f9c7107dc8244cb8d1e8290d30be741"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e417125daf34675943d0734bc52e362"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.1s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69d757ce142a49cba15b553c4ab8735d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1cd9503b6334694a1b7e9230eaa072a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.5s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4861633833d423b9a08a2974973903a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c40ec603f954d3881a58b55f2f5f948"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.3s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e07b5946a0f4eeea2c0dafe453aa992"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576.6865513392861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9543a2ec81a4b70b7b1f1f0fad6526e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.5s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f96d734e63c042f189756c112f6bfa24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35b7636858484e43a9c16f08667a12d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.4s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "507050c3c3ce4843b6e94db18278e155"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   49.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   49.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fe2ab12c53a421bb088640ace8fb60a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.6s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "654294fa25b847a1bbdd67a9b7daf219"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   49.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   49.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5324d16b594f4e95aa4f6287ebecab48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.8s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc7a67b0bd344b769d1ab2d3d7131c02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   49.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   49.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6aad08e11b24656800911f8075e09b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.6s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f633d3bffe934a718e8eac8f7aaee27e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c0a6943e34546868065681f03d8a73f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.7s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "478d91ced2a04e33ae55cd87bff7c033"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0922e658d9cf4c309f1f5e5e97689948"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.7s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f1765362daa4665bc7ac172a468c125"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ae3cd8547c04e1ca4cf19d9139b89b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.9s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fae99f90efb47baad50c76346b9871f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd95b40ea8fc4e4594c5a3c86ee870f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.9s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "635c368e7857483c8481d9f047f3704e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "267331f9c9364393a05cc1063ba25882"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.9s remaining:    0.0s\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 K \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ba2ff9b6b0b48e6a4017058c308f377"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best doubled MAE (CV): 576.6865513392861\n",
      "Best params - LAGS: 5 RNN_LAYERS: 1 GRADIENT_CLIP_VAL: 0.01 SHIFT: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Best doubled MAE (CV): {best_double_MAE}\")\n",
    "print(f\"Best params - LAGS: {BEST_LAGS}\",\n",
    "      f\"RNN_LAYERS: {BEST_RNN_LAYERS}\",\n",
    "      f\"GRADIENT_CLIP_VAL: {BEST_GRADIENT_CLIP_VAL}\",\n",
    "      f\"SHIFT: {BEST_SHIFT}\")\n",
    "plot_backtest(forecast_df=best_forecast, ts=train_ts, history_len=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T14:24:00.618882646Z",
     "start_time": "2023-10-20T14:24:00.297739752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/kukuyura/Downloads/tmp/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 720   \n",
      "4 | distribution_projector | Linear                 | 22    \n",
      "------------------------------------------------------------------\n",
      "742       Trainable params\n",
      "0         Non-trainable params\n",
      "742       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9faf40a444ad47b4a8d831b1817dfe7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:445: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:279: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  future_dates = pd.date_range(\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:293: FutureWarning: The behavior of indexing on a MultiIndex with a nested sequence of labels is deprecated and will change in a future version. `series.loc[label, sequence]` will raise if any members of 'sequence' or not present in the index's second level. To retain the old behavior, use `series.index.isin(sequence, level=1)`\n",
      "  regressors_index = self.df_exog.loc[:, pd.IndexSlice[segment, self.regressors]].index\n",
      "/home/kukuyura/anaconda3/lib/python3.9/site-packages/etna/datasets/tsdataset.py:295: UserWarning: Some regressors don't have enough values in segment main, NaN-s will be used for missing values\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_pipeline.fit(train_ts)\n",
    "forecast_ts = best_pipeline.forecast()\n",
    "forecast_df = forecast_ts.to_pandas()['main'][['target']]\n",
    "forecast_df = forecast_df.rename(columns={'target':'Price'})\n",
    "forecast_df['Date'] = forecast_df.index\n",
    "forecast_df.set_index('Date', inplace=True)\n",
    "forecast_df.to_csv(\"pred.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T14:23:17.370126801Z",
     "start_time": "2023-10-20T14:22:47.824948309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
